{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f994bae2-0698-4f5e-e12c-06d0ff9c7530",
        "id": "l7_z59TzchlU"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100000\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy\n",
        "from tensorflow.python.ops.gen_dataset_ops import shuffle_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset, info = tfds.load(\"movielens/100k-ratings\", split=[\"train\"], with_info=True)\n",
        "\n",
        "data_set = dataset[0]\n",
        "\n",
        "def preprocess_data(dataset):\n",
        "\n",
        "    user_id = tf.strings.to_number(dataset[\"user_id\"], out_type=tf.int32)\n",
        "    movie_id = tf.strings.to_number(dataset[\"movie_id\"], out_type=tf.int32)\n",
        "\n",
        "    #casting user rating into 0 and 1 (0 means didn't rate this movie, 1 means did rate this movie)\n",
        "\n",
        "    did_rate = tf.cast(dataset[\"user_rating\"] > 0, tf.int64)\n",
        "\n",
        "    features = {\n",
        "        \"user_id\": user_id,\n",
        "        \"movie_id\": movie_id,\n",
        "        \"user_rating\":  did_rate\n",
        "    }\n",
        "\n",
        "    return features\n",
        "\n",
        "data_set = data_set.map(preprocess_data)\n",
        "\n",
        "print(len(data_set))\n",
        "\n",
        "num_client = 5\n",
        "userS_dataset = []\n",
        "\n",
        "dataset_size = len(data_set)\n",
        "\n",
        "for i in range(num_client):\n",
        "\n",
        "    seed = numpy.random.seed(i)\n",
        "    data_set = data_set.shuffle(buffer_size=dataset_size, seed= seed)\n",
        "    userS_dataset.append(data_set)\n",
        "\n",
        "data_set_1 =  data_set.shuffle(buffer_size=dataset_size, seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vFIxHjXychlV"
      },
      "outputs": [],
      "source": [
        "#size of training data and testing data\n",
        "train_size = int(dataset_size * 0.8)\n",
        "test_size = dataset_size - train_size\n",
        "\n",
        "#extract training data and testing data\n",
        "train_data_1 = data_set_1.take(int(train_size))\n",
        "test_data_1 = data_set_1.skip(int(train_size))\n",
        "\n",
        "train_data = []\n",
        "test_data = []\n",
        "\n",
        "for i in range(num_client):\n",
        "\n",
        "    train_data.append(userS_dataset[i].take(int(train_size)))\n",
        "    test_data.append(userS_dataset[i].skip(int(train_size)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d236a4e-ae0f-4f06-a005-5e3533962e0c",
        "id": "FsHZCqFZchlW"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique movie pairs: 100000\n",
            "Number of unique users: 943\n",
            "Number of unique movies: 1682\n",
            "Number of ratings: 100000\n"
          ]
        }
      ],
      "source": [
        "userIDs = []\n",
        "movieIDs = []\n",
        "ratings = []\n",
        "\n",
        "unique_user_movie_pair = set()\n",
        "number = 0\n",
        "\n",
        "for example in data_set:\n",
        "\n",
        "    # Convert TensorFlow tensor to a NumPy value\n",
        "    user_id = example[\"user_id\"].numpy()\n",
        "    movie_id = example[\"movie_id\"].numpy()\n",
        "    rating = example[\"user_rating\"].numpy()\n",
        "\n",
        "    # Add movie pair only if it is uniuqe\n",
        "    if (user_id, movie_id) not in unique_user_movie_pair:\n",
        "        userIDs.append(user_id)\n",
        "        movieIDs.append(movie_id)\n",
        "        ratings.append(rating)\n",
        "        unique_user_movie_pair.add((user_id, movie_id))\n",
        "        number += 1\n",
        "\n",
        "print(f\"Number of unique movie pairs: {number}\")\n",
        "\n",
        "# Convert lists to NumPy arrays for later processing\n",
        "userIDs = numpy.array(userIDs)\n",
        "movieIDs = numpy.array(movieIDs)\n",
        "ratings = numpy.array(ratings)\n",
        "\n",
        "movieIDs = numpy.unique(movieIDs)\n",
        "userIDs = numpy.unique(userIDs)\n",
        "\n",
        "#size of user and movie in training data\n",
        "num_user = len(userIDs)\n",
        "num_movie = len(movieIDs)\n",
        "rating = len(ratings)\n",
        "\n",
        "print(f\"Number of unique users: {num_user}\")\n",
        "print(f\"Number of unique movies: {num_movie}\")\n",
        "print(f\"Number of ratings: {rating}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "triplets = list(zip(userIDs, movieIDs, ratings))\n",
        "\n",
        "train_triplets, test_triplets = train_test_split(triplets, test_size=0.2, random_state=42)\n",
        "train_matrix = numpy.zeros((num_user, num_movie), dtype=numpy.int32)\n",
        "test_matrix = numpy.zeros((num_user, num_movie), dtype=numpy.int32)\n",
        "\n",
        "def integrate_feature_into_matrix(userIDs, movieIDs, ratings, num_user, num_movie, matrix):\n",
        "\n",
        "    # Create a 2D matrix filled with zeros\n",
        "    # Populate the matrix\n",
        "    for userID, movieID, rating in zip(userIDs, movieIDs, ratings):\n",
        "        matrix[int(userID), int(movieID)] = rating\n",
        "\n",
        "    return matrix\n",
        "\n",
        "for user_id, movie_id, rating in train_triplets:\n",
        "    train_matrix[user_id-1, movie_id-1] = rating\n",
        "\n",
        "for user_id, movie_id, rating in test_triplets:\n",
        "    test_matrix[user_id-1, movie_id-1] = rating\n",
        "\n",
        "print(train_matrix.shape)\n",
        "print(test_matrix.shape)\n",
        "train_matrix = train_matrix.reshape((1, num_user, num_movie, 1))\n",
        "\n",
        "print(train_matrix.shape)\n",
        "print(test_matrix.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5ccf4bc-4ad8-43e7-e413-78d0a64f473a",
        "id": "WWqzqhGTchlW"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(943, 1682)\n",
            "(943, 1682)\n",
            "(1, 943, 1682, 1)\n",
            "(943, 1682)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train_user, X_test_user, X_train_movie, X_test_movie, y_train, y_test = train_test_split(\n",
        "#     userIDs, movieIDs, ratings, test_size=0.2, random_state=42\n",
        "# )"
      ],
      "metadata": {
        "id": "LwLsN97K7uh2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "648ea5d8-de6f-4f53-92ee-2f79e1e39def",
        "id": "IectNF_UchlW"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/ioloop.py\", line 699, in <lambda>\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/ioloop.py\", line 750, in _run_callback\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 824, in inner\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 785, in run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 249, in wrapper\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 747, in __init__\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 785, in run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 233, in wrapper\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 233, in wrapper\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 233, in wrapper\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-6-fd04bd6b8737>\", line 73, in <cell line: 0>\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\nOut of memory while trying to allocate 6382813184 bytes.\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:   17.85GiB\n              constant allocation:        68B\n        maybe_live_out allocation:   17.84GiB\n     preallocated temp allocation:   11.26GiB\n  preallocated temp fragmentation:       504B (0.00%)\n                 total allocation:   29.11GiB\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 5.94GiB\n\t\tXLA Label: fusion\n\t\tShape: f32[24932864,64]\n\t\t==========================\n\n\tBuffer 2:\n\t\tSize: 5.94GiB\n\t\tOperator: op_name=\"XLA_Args\"\n\t\tEntry Parameter Subshape: f32[24932864,64]\n\t\t==========================\n\n\tBuffer 3:\n\t\tSize: 5.94GiB\n\t\tOperator: op_name=\"XLA_Args\"\n\t\tEntry Parameter Subshape: f32[24932864,64]\n\t\t==========================\n\n\tBuffer 4:\n\t\tSize: 5.94GiB\n\t\tOperator: op_name=\"XLA_Args\"\n\t\tEntry Parameter Subshape: f32[24932864,64]\n\t\t==========================\n\n\tBuffer 5:\n\t\tSize: 769.36MiB\n\t\tOperator: op_type=\"Conv2D\" op_name=\"MoviePopularityModel_1/conv2d_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1196\n\t\tXLA Label: custom-call\n\t\tShape: f32[1,128,939,1678]\n\t\t==========================\n\n\tBuffer 6:\n\t\tSize: 762.98MiB\n\t\tOperator: op_type=\"Conv2DBackpropInput\" op_name=\"MoviePopularityModel_1/conv2d_transpose_1/conv_transpose\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1196\n\t\tXLA Label: custom-call\n\t\tShape: f32[1,128,934,1673]\n\t\t==========================\n\n\tBuffer 7:\n\t\tSize: 761.71MiB\n\t\tOperator: op_type=\"AddV2\" op_name=\"MoviePopularityModel_1/batch_normalization_2_1/batchnorm/add_1\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1196\n\t\tXLA Label: fusion\n\t\tShape: f32[1,128,933,1672]\n\t\t==========================\n\n\tBuffer 8:\n\t\tSize: 382.13MiB\n\t\tOperator: op_type=\"Conv2D\" op_name=\"MoviePopularityModel_1/conv2d_1_2/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1196\n\t\tXLA Label: custom-call\n\t\tShape: f32[1,64,935,1674]\n\t\t==========================\n\n\tBuffer 9:\n\t\tSize: 381.08MiB\n\t\tOperator: op_type=\"Conv2DBackpropInput\" op_name=\"MoviePopularityModel_1/conv2d_transpose_2_1/conv_transpose\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1196\n\t\tXLA Label: custom-call\n\t\tShape: f32[1,64,933,1673]\n\t\t==========================\n\n\tBuffer 10:\n\t\tSize: 381.08MiB\n\t\tOperator: op_type=\"Conv2DBackpropInput\" op_name=\"MoviePopularityModel_1/conv2d_transpose_1_2/conv_transpose\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1196\n\t\tXLA Label: custom-call\n\t\tShape: f32[1,64,933,1673]\n\t\t==========================\n\n\tBuffer 11:\n\t\tSize: 380.85MiB\n\t\tOperator: op_type=\"SelectV2\" op_name=\"MoviePopularityModel_1/dropout_1_2/stateless_dropout/SelectV2\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1196\n\t\tXLA Label: fusion\n\t\tShape: f32[1,64,933,1672]\n\t\t==========================\n\n\tBuffer 12:\n\t\tSize: 380.45MiB\n\t\tOperator: op_type=\"AddV2\" op_name=\"MoviePopularityModel_1/batch_normalization_4_1/batchnorm/add_1\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1196 deduplicated_name=\"loop_add_fusion.3\"\n\t\tXLA Label: fusion\n\t\tShape: f32[1,64,932,1672]\n\t\t==========================\n\n\tBuffer 13:\n\t\tSize: 380.45MiB\n\t\tOperator: op_type=\"AddV2\" op_name=\"MoviePopularityModel_1/batch_normalization_3_1/batchnorm/add_1\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1196 deduplicated_name=\"loop_add_fusion.3\"\n\t\tXLA Label: fusion\n\t\tShape: f32[1,64,932,1672]\n\t\t==========================\n\n\tBuffer 14:\n\t\tSize: 192.02MiB\n\t\tOperator: op_type=\"GreaterEqual\" op_name=\"MoviePopularityModel_1/dropout_1/stateless_dropout/GreaterEqual\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1196\n\t\tXLA Label: fusion\n\t\tShape: pred[128,1573026]\n\t\t==========================\n\n\tBuffer 15:\n\t\tSize: 190.22MiB\n\t\tOperator: op_type=\"SelectV2\" op_name=\"MoviePopularityModel_1/dropout_2_1/stateless_dropout/SelectV2\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1196\n\t\tXLA Label: fusion\n\t\tShape: f32[1,128,466,836]\n\t\t==========================\n\n\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_multi_step_on_iterator_306220]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-fd04bd6b8737>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mtrain_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mtrain_matrix\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# For self-supervised learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mkeras_symbolic_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeras_symbolic_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/ioloop.py\", line 699, in <lambda>\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/ioloop.py\", line 750, in _run_callback\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 824, in inner\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 785, in run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 249, in wrapper\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 747, in __init__\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 785, in run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 233, in wrapper\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 233, in wrapper\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 233, in wrapper\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-6-fd04bd6b8737>\", line 73, in <cell line: 0>\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\nOut of memory while trying to allocate 6382813184 bytes.\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:   17.85GiB\n              constant allocation:        68B\n        maybe_live_out allocation:   17.84GiB\n     preallocated temp allocation:   11.26GiB\n  preallocated temp fragmentation:       504B (0.00%)\n                 total allocation:   29.11GiB\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 5.94GiB\n\t\tXLA Label: fusion\n\t\tShape: f32[24932864,64]\n\t\t==========================\n\n\tBuffer 2:\n\t\tSize: 5.94GiB\n\t\tOperator: op_name=\"XLA_Args\"\n\t\tEntry Parameter Subshape: f32[24932864,64]\n\t\t==========================\n\n\tBuffer 3:\n\t\tSize: 5.94GiB\n\t\tOperator: op_name=\"XLA_Args\"\n\t\tEntry Parameter Subshape: f32[24932864,64]\n\t\t==========================\n\n\tBuffer 4:\n\t\tSize: 5.94GiB\n\t\tOperator: op_name=\"XLA_Args\"\n\t\tEntry Parameter Subshape: f32[24932864,64]\n\t\t==========================\n\n\tBuffer 5:\n\t\tSize: 769.36MiB\n\t\tOperator: op_type=\"Conv2D\" op_name=\"MoviePopularityModel_1/conv2d_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1196\n\t\tXLA Label: custom-call\n\t\tShape: f32[1,128,939,1678]\n\t\t==========================\n\n\tBuffer 6:\n\t\tSize: 762.98MiB\n\t\tOperator: op_type=\"Conv2DBackpropInput\" op_name=\"MoviePopularityModel_1/conv2d_transpose_1/conv_transpose\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1196\n\t\tXLA Label: custom-call\n\t\tShape: f32[1,128,934,1673]\n\t\t==========================\n\n\tBuffer 7:\n\t\tSize: 761.71MiB\n\t\tOperator: op_type=\"AddV2\" op_name=\"MoviePopularityModel_1/batch_normalization_2_1/batchnorm/add_1\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1196\n\t\tXLA Label: fusion\n\t\tShape: f32[1,128,933,1672]\n\t\t==========================\n\n\tBuffer 8:\n\t\tSize: 382.13MiB\n\t\tOperator: op_type=\"Conv2D\" op_name=\"MoviePopularityModel_1/conv2d_1_2/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1196\n\t\tXLA Label: custom-call\n\t\tShape: f32[1,64,935,1674]\n\t\t==========================\n\n\tBuffer 9:\n\t\tSize: 381.08MiB\n\t\tOperator: op_type=\"Conv2DBackpropInput\" op_name=\"MoviePopularityModel_1/conv2d_transpose_2_1/conv_transpose\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1196\n\t\tXLA Label: custom-call\n\t\tShape: f32[1,64,933,1673]\n\t\t==========================\n\n\tBuffer 10:\n\t\tSize: 381.08MiB\n\t\tOperator: op_type=\"Conv2DBackpropInput\" op_name=\"MoviePopularityModel_1/conv2d_transpose_1_2/conv_transpose\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1196\n\t\tXLA Label: custom-call\n\t\tShape: f32[1,64,933,1673]\n\t\t==========================\n\n\tBuffer 11:\n\t\tSize: 380.85MiB\n\t\tOperator: op_type=\"SelectV2\" op_name=\"MoviePopularityModel_1/dropout_1_2/stateless_dropout/SelectV2\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1196\n\t\tXLA Label: fusion\n\t\tShape: f32[1,64,933,1672]\n\t\t==========================\n\n\tBuffer 12:\n\t\tSize: 380.45MiB\n\t\tOperator: op_type=\"AddV2\" op_name=\"MoviePopularityModel_1/batch_normalization_4_1/batchnorm/add_1\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1196 deduplicated_name=\"loop_add_fusion.3\"\n\t\tXLA Label: fusion\n\t\tShape: f32[1,64,932,1672]\n\t\t==========================\n\n\tBuffer 13:\n\t\tSize: 380.45MiB\n\t\tOperator: op_type=\"AddV2\" op_name=\"MoviePopularityModel_1/batch_normalization_3_1/batchnorm/add_1\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1196 deduplicated_name=\"loop_add_fusion.3\"\n\t\tXLA Label: fusion\n\t\tShape: f32[1,64,932,1672]\n\t\t==========================\n\n\tBuffer 14:\n\t\tSize: 192.02MiB\n\t\tOperator: op_type=\"GreaterEqual\" op_name=\"MoviePopularityModel_1/dropout_1/stateless_dropout/GreaterEqual\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1196\n\t\tXLA Label: fusion\n\t\tShape: pred[128,1573026]\n\t\t==========================\n\n\tBuffer 15:\n\t\tSize: 190.22MiB\n\t\tOperator: op_type=\"SelectV2\" op_name=\"MoviePopularityModel_1/dropout_2_1/stateless_dropout/SelectV2\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1196\n\t\tXLA Label: fusion\n\t\tShape: f32[1,128,466,836]\n\t\t==========================\n\n\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_multi_step_on_iterator_306220]"
          ]
        }
      ],
      "source": [
        "def build_cnn_model(num_user, num_movie):\n",
        "\n",
        "    regularization = l2(0.01)\n",
        "\n",
        "    input_matrix = tf.keras.layers.Input(shape=(num_user, num_movie, 1), name=\"user_id\")\n",
        "\n",
        "    # First Convolution Block\n",
        "    cnn_layer = tf.keras.layers.Conv2D(128, kernel_size=(5, 5),strides = (1,1), kernel_regularizer=regularization, use_bias = False)(input_matrix)\n",
        "    cnn_layer = tf.keras.layers.BatchNormalization()(cnn_layer)\n",
        "    cnn_layer = tf.keras.layers.Activation('relu')(cnn_layer)\n",
        "    cnn_layer = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides = (1,1),)(cnn_layer)\n",
        "    cnn_layer = tf.keras.layers.Dropout(0.3)(cnn_layer)\n",
        "\n",
        "    # Second Convolution Block\n",
        "    cnn_layer = tf.keras.layers.Conv2D(64, kernel_size=(4, 4), strides = (1,1), kernel_regularizer=regularization, use_bias = False)(cnn_layer)\n",
        "    cnn_layer = tf.keras.layers.BatchNormalization()(cnn_layer)\n",
        "    cnn_layer = tf.keras.layers.Activation('relu')(cnn_layer)\n",
        "    cnn_layer = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides = (1,1),)(cnn_layer)\n",
        "    cnn_layer = tf.keras.layers.Dropout(0.3)(cnn_layer)\n",
        "\n",
        "    # Third Convolution Block\n",
        "    # cnn_layer = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), strides = (1,1),  kernel_regularizer=regularization, use_bias = False)(cnn_layer)\n",
        "    # cnn_layer = tf.keras.layers.BatchNormalization()(cnn_layer)\n",
        "    # cnn_layer = tf.keras.layers.Activation('relu')(cnn_layer)\n",
        "    # cnn_layer = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides = (2,2),)(cnn_layer)\n",
        "    # cnn_layer = tf.keras.layers.Dropout(0.3)(cnn_layer)\n",
        "\n",
        "    cnn_layer = tf.keras.layers.Conv2DTranspose(128, kernel_size=4, strides=1, padding=\"SAME\", activation=\"relu\")(cnn_layer)\n",
        "    cnn_layer = tf.keras.layers.BatchNormalization()(cnn_layer)\n",
        "    cnn_layer = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides = (2,2),)(cnn_layer)\n",
        "    cnn_layer = tf.keras.layers.Dropout(0.3)(cnn_layer)\n",
        "\n",
        "\n",
        "    cnn_layer = tf.keras.layers.Conv2DTranspose(64, kernel_size=3, strides=2, padding=\"SAME\", activation=\"relu\")(cnn_layer)\n",
        "    cnn_layer = tf.keras.layers.BatchNormalization()(cnn_layer)\n",
        "    cnn_layer = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides = (2,2),)(cnn_layer)\n",
        "    cnn_layer = tf.keras.layers.Dropout(0.3)(cnn_layer)\n",
        "\n",
        "    cnn_layer = tf.keras.layers.Conv2DTranspose(64, kernel_size=3, strides=2, padding=\"SAME\", activation=\"relu\")(cnn_layer)\n",
        "    cnn_layer = tf.keras.layers.BatchNormalization()(cnn_layer)\n",
        "    cnn_layer = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides = (2,2),)(cnn_layer)\n",
        "    cnn_layer = tf.keras.layers.Dropout(0.4)(cnn_layer)\n",
        "\n",
        "    cnn_layer = tf.keras.layers.Flatten()(cnn_layer)\n",
        "\n",
        "    # Dense Layer\n",
        "    dense_layer = tf.keras.layers.Dense(64, activation='relu')(cnn_layer)\n",
        "    dense_layer = tf.keras.layers.Dropout(0.4)(dense_layer)\n",
        "\n",
        "    dense_layer = tf.keras.layers.Dense(64, activation='relu')(dense_layer)\n",
        "    dense_layer = tf.keras.layers.Dropout(0.5)(dense_layer)\n",
        "\n",
        "    # Output Layer\n",
        "    output = tf.keras.layers.Dense(num_movie, activation='sigmoid', name=\"movie_scores\")(dense_layer)\n",
        "\n",
        "    # Build and Compile the Model\n",
        "    model = tf.keras.models.Model(inputs=input_matrix, outputs=output, name=\"MoviePopularityModel\")\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.AdamW(learning_rate=0.001),\n",
        "        loss= \"mse\",  # For regression tasks\n",
        "        metrics=['accuracy']  # Mean Absolute Error for monitoring\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Build the model\n",
        "model = build_cnn_model(num_user, num_movie)\n",
        "\n",
        "#target_train_matrix = train_matrix.reshape((train_matrix, num_movie))\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    train_matrix,\n",
        "    train_matrix,  # For self-supervised learning\n",
        "    epochs= 20,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6f2sQifychlW"
      },
      "outputs": [],
      "source": [
        "test_matrix_reshaped = test_matrix.reshape((1, num_user, num_movie, 1))\n",
        "# Predict movie scores\n",
        "predictions = model.predict(test_matrix_reshaped)\n",
        "\n",
        "# Aggregate scores across users\n",
        "movie_scores = predictions.mean(axis=0)\n",
        "\n",
        "# Get top 50 movie indices\n",
        "# Top 50 in descending ordere\n",
        "movie_score = numpy.argsort(movie_scores)[:][::-1]\n",
        "\n",
        "print(\"Predicted Movies Ranking(by index):\")\n",
        "for rank, movie_idx in enumerate(movie_score, 1):\n",
        "    print(f\"{rank}: Movie Index {movie_idx}, Score {movie_scores[movie_idx] } \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3BVYHoQchlW"
      },
      "outputs": [],
      "source": [
        "# loss, mae = model.evaluate(train_matrix, test_matrix)  # For self-supervised learning, use test_matrix as both inputs and targets\n",
        "# print(f\"Test Loss: {loss}\")\n",
        "# print(f\"Test MAE: {mae}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xyMYl4IchlX"
      },
      "outputs": [],
      "source": [
        "def calculate_caching_hit_rate(predicted_scores, test_matrix, cache_size_array):\n",
        "\n",
        "    \"\"\"\n",
        "    Calculate the caching hit rate.\n",
        "\n",
        "    :param predicted_scores: Array of predicted scores for all movies (shape: num_movies).\n",
        "    :param test_data: Test dataset containing actual movie requests (e.g., movieIDs).\n",
        "    :param cache_size: Number of movies to cache (top-k based on predicted scores).\n",
        "    :return: Hit rate as a float.\n",
        "    \"\"\"\n",
        "\n",
        "    predicted_scores = predicted_scores.flatten()\n",
        "\n",
        "    num_user, num_movie = test_matrix.shape\n",
        "\n",
        "    for cache_size in cache_size_array:\n",
        "\n",
        "      # Get top-k movies based on predicted scores\n",
        "      top_k_movies = numpy.argsort(predicted_scores)[-cache_size:][::-1]\n",
        "\n",
        "      #Extract actual requested movie IDs from test_matrix\n",
        "      requested_movie_ids = set()\n",
        "      hits = 0\n",
        "\n",
        "      for user_id in range(num_user):\n",
        "\n",
        "          requested_movies = numpy.where(test_matrix[user_id] == 1)[0]\n",
        "          requested_movie_ids.update(requested_movies)\n",
        "\n",
        "      # Calculate hit rate\n",
        "      for movie_id in requested_movie_ids:\n",
        "          if movie_id in top_k_movies:\n",
        "              hits += 1\n",
        "\n",
        "      total_unique_movies = len(requested_movie_ids)\n",
        "      print(f\"Total unique movies: {total_unique_movies}, total hit: {hits}\")\n",
        "\n",
        "      hit_rate = hits / cache_size\n",
        "      print(f\"Hit Rate for cache size {cache_size}: {hit_rate:.2%}\")\n",
        "      print(\"\")\n",
        "\n",
        "    return hit_rate\n",
        "\n",
        "# Example usage\n",
        "cache_size_array = [50, 100, 150, 200, 250, 300]  # the number of cached top N movies\n",
        "hit_rate = calculate_caching_hit_rate(predictions, test_matrix, cache_size_array)"
      ]
    }
  ]
}