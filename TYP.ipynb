{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "651a6e70-87c6-46de-8617-fb712233a5c9",
        "id": "l7_z59TzchlU"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100000\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy\n",
        "from tensorflow.python.ops.gen_dataset_ops import shuffle_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset, info = tfds.load(\"movielens/100k-ratings\", split=[\"train\"], with_info=True)\n",
        "\n",
        "data_set = dataset[0]\n",
        "\n",
        "def preprocess_data(dataset):\n",
        "\n",
        "    user_id = tf.strings.to_number(dataset[\"user_id\"], out_type=tf.int32)\n",
        "    movie_id = tf.strings.to_number(dataset[\"movie_id\"], out_type=tf.int32)\n",
        "\n",
        "    #casting user rating into 0 and 1 (0 means didn't rate this movie, 1 means did rate this movie)\n",
        "\n",
        "    did_rate = tf.cast(dataset[\"user_rating\"] > 0, tf.int64)\n",
        "\n",
        "    features = {\n",
        "        \"user_id\": user_id,\n",
        "        \"movie_id\": movie_id,\n",
        "        \"user_rating\":  did_rate\n",
        "    }\n",
        "\n",
        "    return features\n",
        "\n",
        "data_set = data_set.map(preprocess_data)\n",
        "\n",
        "print(len(data_set))\n",
        "\n",
        "num_client = 5\n",
        "userS_dataset = []\n",
        "\n",
        "dataset_size = len(data_set)\n",
        "\n",
        "for i in range(num_client):\n",
        "\n",
        "    seed = numpy.random.seed(i)\n",
        "    data_set = data_set.shuffle(buffer_size=dataset_size, seed= seed)\n",
        "    userS_dataset.append(data_set)\n",
        "\n",
        "data_set_1 =  data_set.shuffle(buffer_size=dataset_size, seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "vFIxHjXychlV"
      },
      "outputs": [],
      "source": [
        "#size of training data and testing data\n",
        "train_size = int(dataset_size * 0.8)\n",
        "test_size = dataset_size - train_size\n",
        "\n",
        "#extract training data and testing data\n",
        "train_data_1 = data_set_1.take(int(train_size))\n",
        "test_data_1 = data_set_1.skip(int(train_size))\n",
        "\n",
        "train_data = []\n",
        "test_data = []\n",
        "\n",
        "for i in range(num_client):\n",
        "\n",
        "    train_data.append(userS_dataset[i].take(int(train_size)))\n",
        "    test_data.append(userS_dataset[i].skip(int(train_size)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60716feb-75c4-4775-85a2-c2a1deca7c76",
        "id": "FsHZCqFZchlW"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique movie pairs: 100000\n",
            "Number of unique users: 943\n",
            "Number of unique movies: 1682\n"
          ]
        }
      ],
      "source": [
        "userIDs = []\n",
        "movieIDs = []\n",
        "ratings = []\n",
        "\n",
        "unique_user_movie_pair = set()\n",
        "number = 0\n",
        "\n",
        "for example in data_set:\n",
        "\n",
        "    # Convert TensorFlow tensor to a NumPy value\n",
        "    user_id = example[\"user_id\"].numpy()\n",
        "    movie_id = example[\"movie_id\"].numpy()\n",
        "    rating = example[\"user_rating\"].numpy()\n",
        "\n",
        "    # Add movie pair only if it is uniuqe\n",
        "    if (user_id, movie_id) not in unique_user_movie_pair:\n",
        "        userIDs.append(user_id)\n",
        "        movieIDs.append(movie_id)\n",
        "        ratings.append(rating)\n",
        "        unique_user_movie_pair.add((user_id, movie_id))\n",
        "        number += 1\n",
        "\n",
        "print(f\"Number of unique movie pairs: {number}\")\n",
        "\n",
        "# Convert lists to NumPy arrays for later processing\n",
        "userIDs = numpy.array(userIDs)\n",
        "movieIDs = numpy.array(movieIDs)\n",
        "ratings = numpy.array(ratings)\n",
        "\n",
        "movieIDs = numpy.unique(movieIDs)\n",
        "userIDs = numpy.unique(userIDs)\n",
        "\n",
        "#size of user and movie in training data\n",
        "num_user = max(userIDs)\n",
        "num_movie = max(movieIDs)\n",
        "\n",
        "print(f\"Number of unique users: {num_user}\")\n",
        "print(f\"Number of unique movies: {num_movie}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "triplets = list(zip(userIDs, movieIDs, ratings))\n",
        "\n",
        "train_triplets, test_triplets = train_test_split(triplets, test_size=0.2, random_state=42)\n",
        "train_matrix = numpy.zeros((num_user, num_movie), dtype=numpy.int32)\n",
        "test_matrix = numpy.zeros((num_user, num_movie), dtype=numpy.int32)\n",
        "\n",
        "def integrate_feature_into_matrix(userIDs, movieIDs, ratings, num_user, num_movie, matrix):\n",
        "\n",
        "    # Create a 2D matrix filled with zeros\n",
        "    # Populate the matrix\n",
        "    for userID, movieID, rating in zip(userIDs, movieIDs, ratings):\n",
        "        matrix[int(userID), int(movieID)] = rating\n",
        "\n",
        "    return matrix\n",
        "\n",
        "for user_id, movie_id, rating in train_triplets:\n",
        "    train_matrix[user_id-1, movie_id-1] = rating\n",
        "\n",
        "for user_id, movie_id, rating in test_triplets:\n",
        "    test_matrix[user_id-1, movie_id-1] = rating\n",
        "\n",
        "print(train_matrix.shape)\n",
        "print(test_matrix.shape)\n",
        "train_matrix = train_matrix.reshape((1, num_user, num_movie, 1))\n",
        "\n",
        "print(train_matrix.shape)\n",
        "print(test_matrix.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aa69445-c74b-4269-a832-6f4a9f88c19d",
        "id": "WWqzqhGTchlW"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(943, 1682)\n",
            "(943, 1682)\n",
            "(1, 943, 1682, 1)\n",
            "(943, 1682)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "bf212c31-5fb9-4be0-cade-c4214f7fa310",
        "id": "IectNF_UchlW"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Data cardinality is ambiguous. Make sure all arrays contain the same number of samples.'x' sizes: 1\n'y' sizes: 943\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-f6f44a8b7651>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mtrain_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mtarget_train_matrix\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# For self-supervised learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/data_adapter_utils.py\u001b[0m in \u001b[0;36mcheck_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    113\u001b[0m             )\n\u001b[1;32m    114\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"'{label}' sizes: {sizes}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous. Make sure all arrays contain the same number of samples.'x' sizes: 1\n'y' sizes: 943\n"
          ]
        }
      ],
      "source": [
        "def build_cnn_model(num_user, num_movie):\n",
        "\n",
        "    regularization = l2(0.01)\n",
        "\n",
        "    input_matrix = tf.keras.layers.Input(shape=(num_user, num_movie, 1), name=\"user_id\")\n",
        "\n",
        "    # First Convolution Block\n",
        "    cnn_layer = tf.keras.layers.Conv2D(32, kernel_size=(5, 5), kernel_regularizer=regularization)(input_matrix)\n",
        "    cnn_layer = tf.keras.layers.Activation('relu')(cnn_layer)\n",
        "    cnn_layer = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(cnn_layer)\n",
        "    cnn_layer = tf.keras.layers.Dropout(0.1)(cnn_layer)\n",
        "\n",
        "    # Second Convolution Block\n",
        "    cnn_layer = tf.keras.layers.Conv2D(64, kernel_size=(4, 4), kernel_regularizer=regularization)(cnn_layer)\n",
        "    cnn_layer = tf.keras.layers.Activation('relu')(cnn_layer)\n",
        "    cnn_layer = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(cnn_layer)\n",
        "    cnn_layer = tf.keras.layers.Dropout(0.15)(cnn_layer)\n",
        "\n",
        "    # Third Convolution Block\n",
        "    cnn_layer = tf.keras.layers.Conv2D(128, kernel_size=(3, 3), kernel_regularizer=regularization)(cnn_layer)\n",
        "    cnn_layer = tf.keras.layers.Activation('relu')(cnn_layer)\n",
        "    cnn_layer = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(cnn_layer)\n",
        "    cnn_layer = tf.keras.layers.Dropout(0.2)(cnn_layer)\n",
        "\n",
        "    cnn_layer = tf.keras.layers.Flatten()(cnn_layer)\n",
        "\n",
        "    # Dense Layer\n",
        "    dense_layer = tf.keras.layers.Dense(64, activation='relu')(cnn_layer)\n",
        "    dense_layer = tf.keras.layers.Dropout(0.25)(dense_layer)\n",
        "\n",
        "    dense_layer = tf.keras.layers.Dense(128, activation='relu')(dense_layer)\n",
        "    dense_layer = tf.keras.layers.Dropout(0.2)(dense_layer)\n",
        "\n",
        "    # Output Layer\n",
        "    output = tf.keras.layers.Dense(num_movie, activation='sigmoid', name=\"movie_scores\")(dense_layer)\n",
        "\n",
        "    # Build and Compile the Model\n",
        "    model = tf.keras.models.Model(inputs=input_matrix, outputs=output, name=\"MoviePopularityModel\")\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.AdamW(learning_rate=0.001),\n",
        "        loss= \"binary_crossentropy\",  # For regression tasks\n",
        "        metrics=['accuracy']  # Mean Absolute Error for monitoring\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# def bpr_loss(y_true, y_pred):\n",
        "\n",
        "#     # y_true: Binary mask indicating rated movies (1) vs. unrated (0)\n",
        "#     # y_pred: Predicted scores for movies\n",
        "\n",
        "#     # Extract positive (rated) and negative (unrated) movie scores\n",
        "#     positive_scores = tf.boolean_mask(y_pred, y_true == 1)\n",
        "#     negative_scores = tf.boolean_mask(y_pred, y_true == 0)\n",
        "\n",
        "#     # Compute BPR loss\n",
        "#     loss = -tf.reduce_mean(tf.math.log_sigmoid(positive_scores - negative_scores))\n",
        "#     return loss\n",
        "\n",
        "# Build the model\n",
        "model = build_cnn_model(num_user, num_movie)\n",
        "\n",
        "target_train_matrix = tf.reshape((train_matrix, num_movie))\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    train_matrix,\n",
        "    target_train_matrix,  # For self-supervised learning\n",
        "    epochs=20,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6f2sQifychlW"
      },
      "outputs": [],
      "source": [
        "test_matrix_reshaped = test_matrix.reshape((1, num_user, num_movie, 1))\n",
        "# Predict movie scores\n",
        "predictions = model.predict(test_matrix_reshaped)\n",
        "\n",
        "# Aggregate scores across users\n",
        "movie_scores = predictions.mean(axis=0)\n",
        "\n",
        "# Get top 50 movie indices\n",
        "# Top 50 in descending ordere\n",
        "movie_score = numpy.argsort(movie_scores)[:][::-1]\n",
        "\n",
        "print(\"Predicted Movies Ranking(by index):\")\n",
        "for rank, movie_idx in enumerate(movie_score, 1):\n",
        "    print(f\"{rank}: Movie Index {movie_idx}, Score {movie_scores[movie_idx] } \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3BVYHoQchlW"
      },
      "outputs": [],
      "source": [
        "# loss, mae = model.evaluate(train_matrix, test_matrix)  # For self-supervised learning, use test_matrix as both inputs and targets\n",
        "# print(f\"Test Loss: {loss}\")\n",
        "# print(f\"Test MAE: {mae}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xyMYl4IchlX"
      },
      "outputs": [],
      "source": [
        "def calculate_caching_hit_rate(predicted_scores, test_matrix, cache_size_array):\n",
        "\n",
        "    \"\"\"\n",
        "    Calculate the caching hit rate.\n",
        "\n",
        "    :param predicted_scores: Array of predicted scores for all movies (shape: num_movies).\n",
        "    :param test_data: Test dataset containing actual movie requests (e.g., movieIDs).\n",
        "    :param cache_size: Number of movies to cache (top-k based on predicted scores).\n",
        "    :return: Hit rate as a float.\n",
        "    \"\"\"\n",
        "\n",
        "    predicted_scores = predicted_scores.flatten()\n",
        "\n",
        "    num_user, num_movie = test_matrix.shape\n",
        "\n",
        "    for cache_size in cache_size_array:\n",
        "\n",
        "      # Get top-k movies based on predicted scores\n",
        "      top_k_movies = numpy.argsort(predicted_scores)[-cache_size:][::-1]\n",
        "\n",
        "      #Extract actual requested movie IDs from test_matrix\n",
        "      requested_movie_ids = set()\n",
        "      hits = 0\n",
        "\n",
        "      for user_id in range(num_user):\n",
        "\n",
        "          requested_movies = numpy.where(test_matrix[user_id] == 1)[0]\n",
        "          requested_movie_ids.update(requested_movies-1)\n",
        "\n",
        "      # Calculate hit rate\n",
        "      for movie_id in requested_movie_ids:\n",
        "          if movie_id in top_k_movies:\n",
        "              hits += 1\n",
        "\n",
        "      total_unique_movies = len(requested_movie_ids)\n",
        "      print(f\"Total unique movies: {total_unique_movies}, total hit: {hits}\")\n",
        "\n",
        "      hit_rate = hits / cache_size\n",
        "      print(f\"Hit Rate for cache size {cache_size}: {hit_rate:.2%}\")\n",
        "      print(\"\")\n",
        "\n",
        "    return hit_rate\n",
        "\n",
        "# Example usage\n",
        "cache_size_array = [50, 100, 150, 200, 250, 300]  # the number of cached top N movies\n",
        "hit_rate = calculate_caching_hit_rate(predictions, test_matrix, cache_size_array)"
      ]
    }
  ]
}